{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_IhA6gyAWIY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch nltk evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6ZDQO-e9PPI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import evaluate\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Load the Qwen model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIe3or_89bQi"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Define Shannon entropy calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZqOe7zd9ejR"
      },
      "outputs": [],
      "source": [
        "def shannon_entropy(probabilities):\n",
        "    \"\"\"\n",
        "    Calculate Shannon entropy for a probability distribution.\n",
        "    \"\"\"\n",
        "    return -np.sum(probabilities * np.log(probabilities + 1e-10))  # Add small epsilon to avoid log(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Implement entropy-controlled text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll-I_CWY9o8M"
      },
      "outputs": [],
      "source": [
        "def generate_text_with_entropy_control(prompt, max_length=50, target_entropy=1.0, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text using entropy-controlled decoding with the Qwen model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Input text or instruction to start generation.\n",
        "        max_length (int): Maximum length of the generated text.\n",
        "        target_entropy (float): Desired entropy level for diversity control.\n",
        "        temperature (float): Sampling temperature (higher = more random).\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text.\n",
        "    \"\"\"\n",
        "    # Format the input as an instruction (optional, depending on the model)\n",
        "    formatted_prompt = f\"Instruction: {prompt}\\nOutput:\"\n",
        "    input_ids = tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
        "    generated_text = formatted_prompt\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Get model outputs\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :] / temperature  # Apply temperature scaling\n",
        "\n",
        "        # Compute probabilities\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze().numpy()\n",
        "\n",
        "        # Calculate entropy\n",
        "        entropy = shannon_entropy(probs)\n",
        "\n",
        "        # Adjust sampling based on target entropy\n",
        "        if entropy < target_entropy:\n",
        "            # Encourage diversity (higher entropy)\n",
        "            probs = probs ** (1 / temperature)\n",
        "            probs = probs / np.sum(probs)  # Renormalize\n",
        "        else:\n",
        "            # Encourage coherence (lower entropy)\n",
        "            probs = probs ** temperature\n",
        "            probs = probs / np.sum(probs)  # Renormalize\n",
        "\n",
        "        # Sample the next token\n",
        "        next_token_id = np.random.choice(len(probs), p=probs)\n",
        "        next_token = tokenizer.decode([next_token_id])\n",
        "\n",
        "        # Append the token to the generated text\n",
        "        generated_text += next_token\n",
        "\n",
        "        # Update input_ids for the next step\n",
        "        input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]])], dim=1)\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 6: Define evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYCmjWS59sg_"
      },
      "outputs": [],
      "source": [
        "# Perplexity: Measures how well the model predicts the text\n",
        "def calculate_perplexity(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The generated text.\n",
        "        model: The language model.\n",
        "        tokenizer: The tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        float: The perplexity score.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Calculate perplexity\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        perplexity = torch.exp(loss).item()\n",
        "\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mTNkTJzgN2zp"
      },
      "outputs": [],
      "source": [
        "def distinct_n(text, n=2):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    n_grams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "    unique_n_grams = Counter(n_grams)\n",
        "    return len(unique_n_grams) / len(n_grams) if len(n_grams) > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YwQyPc85N7XK"
      },
      "outputs": [],
      "source": [
        "def repetition_rate(text, n=2):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    n_grams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "    n_gram_counts = Counter(n_grams)\n",
        "    repeated = sum(count > 1 for count in n_gram_counts.values())\n",
        "    return repeated / len(n_gram_counts) if len(n_gram_counts) > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 7: Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUxwq8UT9xyz"
      },
      "outputs": [],
      "source": [
        "# Example prompt\n",
        "prompt = \"Write a short story about a robot learning to paint.\"\n",
        "\n",
        "# Generate text with low entropy (more deterministic)\n",
        "low_entropy_text = generate_text_with_entropy_control(prompt, max_length=100, target_entropy=0.5)\n",
        "\n",
        "# Generate text with high entropy (more diverse)\n",
        "high_entropy_text = generate_text_with_entropy_control(prompt, max_length=100, target_entropy=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean the generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Z6U7r3CJM9uT"
      },
      "outputs": [],
      "source": [
        "def clean_generated_text(generated_text):\n",
        "    \"\"\"\n",
        "    Extract the text after \"Output:\" in the generated text.\n",
        "\n",
        "    Args:\n",
        "        generated_text (str): The full generated text including the prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned generated text.\n",
        "    \"\"\"\n",
        "    # Split the text by \"Output:\" and take the second part\n",
        "    if \"Output:\" in generated_text:\n",
        "        return generated_text.split(\"Output:\")[1].strip()\n",
        "    return generated_text  # Fallback if \"Output:\" is not found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# evaluate the cleaned generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lZXCuFFNER-"
      },
      "outputs": [],
      "source": [
        "cleaned_low_entropy_text = clean_generated_text(low_entropy_text)\n",
        "cleaned_high_entropy_text = clean_generated_text(high_entropy_text)\n",
        "\n",
        "print(\"Cleaned Low Entropy Text:\")\n",
        "print(cleaned_low_entropy_text)\n",
        "\n",
        "print(\"Cleaned High Entropy Text:\")\n",
        "print(cleaned_high_entropy_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVzVnnJs96RS",
        "outputId": "a9e1958f-509d-47e2-da42-e34f35566413"
      },
      "outputs": [],
      "source": [
        "print(\"Distinct-2 (Low Entropy):\", distinct_n(cleaned_low_entropy_text, n=2))\n",
        "print(\"Distinct-2 (High Entropy):\", distinct_n(cleaned_high_entropy_text, n=2))\n",
        "\n",
        "print(\"Repetition Rate (Low Entropy):\", repetition_rate(cleaned_low_entropy_text, n=2))\n",
        "print(\"Repetition Rate (High Entropy):\", repetition_rate(cleaned_high_entropy_text, n=2))\n",
        "\n",
        "print(\"Perplexity (Low Entropy):\", calculate_perplexity(cleaned_low_entropy_text, model, tokenizer))\n",
        "print(\"Perplexity (High Entropy):\", calculate_perplexity(cleaned_high_entropy_text, model, tokenizer))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
